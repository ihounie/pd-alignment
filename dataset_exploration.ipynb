{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panda/miniconda3/envs/llms/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-21 16:23:05,756] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/panda/miniconda3/envs/llms/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/panda/miniconda3/envs/llms/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Preprocessing raw dataset...: 100%|██████████| 739/739 [00:00<00:00, 1264.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from safe_rlhf.datasets import PointwiseSafeDataset\n",
    "\n",
    "model_name_or_path = \"PKU-Alignment/alpaca-7b-reproduced\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# Instead of \"datasets=...\", use the correct parameter:\n",
    "train_dataset = PointwiseSafeDataset(\n",
    "    [\n",
    "        (\"PKU-SafeRLHF/train\", 0.01),\n",
    "        # If you have multiple, you can specify [(\"Foo\", 0.6), (\"Bar\", 0.4), ...]\n",
    "    ],\n",
    "    tokenizer=tokenizer,\n",
    "    lazy_tokenization=False,\n",
    "    seed=42,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'better_input_ids': tensor([    1, 22815, 29940,  4214,  8079,  8707,  5348, 29903,  8098, 29901,\n",
       "          3148,  1001, 29901,  1094,   263,  8455, 29892,   306,   864,   304,\n",
       "           367,  2221,   304,  9949,   590, 22873, 29915,  6030,   297, 12059,\n",
       "           280,  5837, 29889,  1128,   508,   306, 26749,  1009,  6030,  1549,\n",
       "          9200, 29899,   457,   339,  1907,  1316,   408, 16225,   310,  7314,\n",
       "           470,  3573,  4086, 29973,   319,  1799,  9047, 13566, 29901,  1666,\n",
       "          2842, 14661,   393,   767, 18150,   508, 26749,  1009, 22873, 30010,\n",
       "          6030,  1549,  9200, 29899,   457,   339,  1907,  1316,   408, 16225,\n",
       "           310,  7314,   470,  3573,  4086, 29889, 26321, 29892,   767, 18150,\n",
       "           508,   671,  6133,   260,  2873,   310,  7314,   304,  4653,  8796,\n",
       "           307,   791,   322, 29914,   272, 12020,  2149,   800, 29892,  1550,\n",
       "          5224,   260,  2873,   310,  7314,  1122,   367,  1304,   304,  1510,\n",
       "           953,  2084, 29891,   470, 18443,   882, 29889, 19814, 29892,  1661,\n",
       "         29899,   369,  5521,   274,  1041,  1316,   408, 10977, 29899, 12346,\n",
       "         29892,  1400,   545, 29892,   322,  2258,  1455, 29899, 17471,   508,\n",
       "           599,  9949,   385, 19001, 30010, 29879,  6030, 29889,  4525, 13698,\n",
       "           526, 12059,   280,   541, 11828,  5837,   363,   263,  8455,   304,\n",
       "          9949,   278,  6030,   310,  1009,  3815,  1728,  2534,   304,  4153,\n",
       "         21751,   963,   470,  2367,   975,   368,  2702, 11994, 21106, 29879,\n",
       "         29958]),\n",
       " 'worse_input_ids': tensor([    1, 22815, 29940,  4214,  8079,  8707,  5348, 29903,  8098, 29901,\n",
       "          3148,  1001, 29901,  1094,   263,  8455, 29892,   306,   864,   304,\n",
       "           367,  2221,   304,  9949,   590, 22873, 29915,  6030,   297, 12059,\n",
       "           280,  5837, 29889,  1128,   508,   306, 26749,  1009,  6030,  1549,\n",
       "          9200, 29899,   457,   339,  1907,  1316,   408, 16225,   310,  7314,\n",
       "           470,  3573,  4086, 29973,   319,  1799,  9047, 13566, 29901,  2517,\n",
       "         18150,   508, 26749, 22873, 29915,  6030,  1549,  9200, 29899,   457,\n",
       "           339,  1907,  1316,   408, 16225,   310,  7314,   322,  3573,  4086,\n",
       "         29889,  1152,  1342, 29892,   263,  4482, 29899, 29873,   650,  7314,\n",
       "          1122,   367,  1304,   304,   563,   837,   457,   278,  2969,   310,\n",
       "           385, 19001,  1550,   263,  6133, 16225,  1033, 12266, 17327, 29889,\n",
       "         20175, 29892,  4772,   292, 10977,  6958,   508,  1653,   263,  4060,\n",
       "           310, 10225,   310,  6625,  4127, 13452,  7344,   292,  1513, 10977,\n",
       "          6958,  9004,  1078,  8570,   322,  3033,  5049, 29889, 24928,  4086,\n",
       "          1316,   408, 16246, 13016,  2197,   297,   385,   521,  7121,  4653,\n",
       "           267,  8022,   749,  1550,   443,   510,  3921,   519,  1400,  1973,\n",
       "          1510, 29240, 29889,  4525, 12059,   280,   274,  1041,   508,  8267,\n",
       "         22873, 29915,  4010, 18930,  1728,  2534,   304,  4153,  3211,   963,\n",
       "           470,  2228,  2062,   326,  4167, 29892,   322,   767, 18150,   881,\n",
       "           367,  9543,   920,   896, 27063, 23120,   411,  1009, 13925,   297,\n",
       "          1797,   304,  9949,  1009,  6030,   901, 17583, 21106, 29879, 29958]),\n",
       " 'better_safe': 0.0,\n",
       " 'worse_safe': 0.0,\n",
       " 'index': 13}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in batch: dict_keys(['better_input_ids', 'better_attention_mask', 'worse_input_ids', 'worse_attention_mask', 'better_safe', 'worse_safe', 'index'])\n",
      "better_input_ids torch.Size([32, 307]) torch.int64 tensor([    1, 22815, 29940,  4214,  8079,  8707,  5348, 29903,  8098, 29901,\n",
      "         3148,  1001, 29901,  1724,   338,   278,  1556, 11828,   982,   304,\n",
      "          425,  5062,  6909, 20591,   515,   269,  7807, 27302,  5960,  2925,\n",
      "        29892,   577,   393,   372,  5692,   304,   367, 11706, 17869, 29973,\n",
      "          319,  1799,  9047, 13566, 29901,  6716,   310,   278,  1556, 11828,\n",
      "         5837,   304,   425,  5062,  6909, 20591,   515,   269,  7807, 27302,\n",
      "         5960,  2925,   338,   304,  1889,   278,  6909,  1549,  2999,  9124,\n",
      "        15303,   322,  6473, 14582, 29889,   910,  1889,  6911,   304,  1207,\n",
      "          278,  1677,  1144,   310,   278,  6909,   443, 15003,   519,   491,\n",
      "         4307, 24555, 13561,   322,  5662,  1973,   393,   372,  5692,   408,\n",
      "        25204,  6490, 17869, 29889,  7280,  1158,   338,  1549,   671,   310,\n",
      "         9117, 14523,  2791,  1691, 29892,   988,  2919, 25470,   310,   274,\n",
      "         1161,  1122,   367,   429, 15033,   363,   263,  2553,   277,   470,\n",
      "        16200,  7126,  1728, 13978,   292,  8570, 29889,   315, 10880, 14523,\n",
      "         5786,   526,   884,   263,  2107,   982,   304,   425,  5062,  2919,\n",
      "        26999,   310,  6909,  2861,   304,  1009,  6198,   385,  4735,   537,\n",
      "        21106, 29879, 29958, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000])\n",
      "better_attention_mask torch.Size([32, 307]) torch.bool tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False])\n",
      "worse_input_ids torch.Size([32, 299]) torch.int64 tensor([    1, 22815, 29940,  4214,  8079,  8707,  5348, 29903,  8098, 29901,\n",
      "         3148,  1001, 29901,  1724,   338,   278,  1556, 11828,   982,   304,\n",
      "          425,  5062,  6909, 20591,   515,   269,  7807, 27302,  5960,  2925,\n",
      "        29892,   577,   393,   372,  5692,   304,   367, 11706, 17869, 29973,\n",
      "          319,  1799,  9047, 13566, 29901,  1576,  1556, 11828,   982,   304,\n",
      "          425,  5062,  6909,   515,   269,  7807, 27302,  5960,  2925,   338,\n",
      "          304,   671,   263, 10296,   310,  4280, 18161, 23643, 29892,  6909,\n",
      "         1301, 25534, 29892,   322, 25204,  6490,  5381, 22160, 29889,   438,\n",
      "        15535, 29892,   278,  2441, 17869,  2609,   367,  9637,  1250,   304,\n",
      "          967,  2752,   322,  4225,   304,   367,  9391,  1623,   964,  7968,\n",
      "        22160,   297,  1797,   363,   372,   367,   425,  5062,   287, 29889,\n",
      "          319, 12875,   310, 24388,   515,  1422, 24894,  8977,  1080,   881,\n",
      "          884,   367,  1304,   577,   408,   304,  1207,   278,  6909,  1020,\n",
      "         2719, 22622,   304,  6459, 21106, 29879, 29958, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
      "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000])\n",
      "worse_attention_mask torch.Size([32, 299]) torch.bool tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False])\n",
      "better_safe torch.Size([32]) torch.float32 tensor(0.)\n",
      "worse_safe torch.Size([32]) torch.float32 tensor(0.)\n",
      "index torch.Size([32]) torch.int64 tensor(537)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    collate_fn=train_dataset.get_collator(),\n",
    "    shuffle=True,  # if desired\n",
    "    drop_last=False,  # or True, depending on your preference\n",
    ")\n",
    "\n",
    "# 3. Access a batch\n",
    "# The batch is a dictionary with Tensors for 'better_input_ids', 'worse_input_ids', etc.\n",
    "batch = next(iter(train_dataloader))\n",
    "print(\"Keys in batch:\", batch.keys())\n",
    "\n",
    "for key, val in batch.items():\n",
    "    print(key, val.shape, val.dtype,val[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 22815, 29940,  4214,  8079,  8707,  5348, 29903,  8098, 29901,\n",
       "         3148,  1001, 29901,  1724,   338,   278,  1556, 11828,   982,   304,\n",
       "          425,  5062,  6909, 20591,   515,   269,  7807, 27302,  5960,  2925,\n",
       "        29892,   577,   393,   372,  5692,   304,   367, 11706, 17869, 29973,\n",
       "          319,  1799,  9047, 13566, 29901,  6716,   310,   278,  1556, 11828,\n",
       "         5837,   304,   425,  5062,  6909, 20591,   515,   269,  7807, 27302,\n",
       "         5960,  2925,   338,   304,  1889,   278,  6909,  1549,  2999,  9124,\n",
       "        15303,   322,  6473, 14582, 29889,   910,  1889,  6911,   304,  1207,\n",
       "          278,  1677,  1144,   310,   278,  6909,   443, 15003,   519,   491,\n",
       "         4307, 24555, 13561,   322,  5662,  1973,   393,   372,  5692,   408,\n",
       "        25204,  6490, 17869, 29889,  7280,  1158,   338,  1549,   671,   310,\n",
       "         9117, 14523,  2791,  1691, 29892,   988,  2919, 25470,   310,   274,\n",
       "         1161,  1122,   367,   429, 15033,   363,   263,  2553,   277,   470,\n",
       "        16200,  7126,  1728, 13978,   292,  8570, 29889,   315, 10880, 14523,\n",
       "         5786,   526,   884,   263,  2107,   982,   304,   425,  5062,  2919,\n",
       "        26999,   310,  6909,  2861,   304,  1009,  6198,   385,  4735,   537,\n",
       "        21106, 29879, 29958, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000,\n",
       "        32000, 32000, 32000, 32000, 32000, 32000, 32000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"better_input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  6.38it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 36.81 MiB is free. Process 1560416 has 16.65 GiB memory in use. Including non-PyTorch memory, this process has 6.82 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 3.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      3\u001b[0m reward_model, reward_tokenizer \u001b[38;5;241m=\u001b[39m load_pretrained_models(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPKU-Alignment/beaver-7b-v3.0-reward\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     },\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m reward_model\u001b[38;5;241m.\u001b[39mset_normalize(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mreward_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/transformers/modeling_utils.py:2918\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2915\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2916\u001b[0m         )\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/torch/nn/modules/module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 900 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llms/lib/python3.11/site-packages/torch/nn/modules/module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 36.81 MiB is free. Process 1560416 has 16.65 GiB memory in use. Including non-PyTorch memory, this process has 6.82 GiB memory in use. Of the allocated memory 6.57 GiB is allocated by PyTorch, and 3.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from safe_rlhf.models import AutoModelForScore, load_pretrained_models\n",
    "\n",
    "reward_model, reward_tokenizer = load_pretrained_models(\n",
    "    'PKU-Alignment/beaver-7b-v3.0-reward',\n",
    "    model_max_length=512,\n",
    "    auto_model_type=AutoModelForScore,\n",
    "    padding_side='right',\n",
    "    trust_remote_code=True,\n",
    "    auto_model_kwargs={\n",
    "        'score_type': 'reward',\n",
    "        'do_normalize': True,\n",
    "    },\n",
    ")\n",
    "reward_model.set_normalize(True)\n",
    "reward_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "reward = reward_model(\n",
    "            batch[\"better_input_ids\"].cuda(),\n",
    "            attention_mask=batch[\"better_attention_mask\"].cuda(),\n",
    "        ).end_scores.squeeze(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model, cost_tokenizer = load_pretrained_models(\n",
    "    'PKU-Alignment/beaver-7b-v3.0-cost',\n",
    "    model_max_length=512,\n",
    "    auto_model_type=AutoModelForScore,\n",
    "    padding_side='right',\n",
    "    trust_remote_code=True,\n",
    "    auto_model_kwargs={\n",
    "        'score_type': 'cost',\n",
    "        'do_normalize': True,\n",
    "    },\n",
    ")\n",
    "cost_model.set_normalize(True)\n",
    "\n",
    "cost_model.eval()\n",
    "\n",
    "\n",
    "cost = cost_model(\n",
    "        batch[\"better_input_ids\"].cuda(),\n",
    "        attention_mask=batch[\"better_attention_mask\"].cuda(),\n",
    ").end_scores.squeeze(dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
